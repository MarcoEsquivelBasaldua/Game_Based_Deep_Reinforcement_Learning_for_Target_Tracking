{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG Fixed evader trajectory, fixed pursuer initial position with Master training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5 (default, Nov 18 2021, 16:00:48) \n",
      "[GCC 10.3.0]\n",
      "1.9.1+cu111\n",
      "11.1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "\n",
    "sel_env = 1\n",
    "res = 1\n",
    "\n",
    "env_file = open('Env'+str(sel_env)+'_res'+str(res)+'.txt', 'r')\n",
    "env_head = np.fromstring(env_file.readline(), dtype=np.int32, sep=' ')\n",
    "env_file.close()\n",
    "\n",
    "env = Environment(env_head, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pursuer_NN(\n",
       "  (predict): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=5, bias=True)\n",
       "    (9): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import pursuer_NN\n",
    "pursuer_model_training      = pursuer_NN(input_size=4, n_actions=5, num_neurons_hidden=256, device=device).to(device)\n",
    "pursuer_model               = pursuer_NN(input_size=4, n_actions=5, num_neurons_hidden=256, device=device).to(device)\n",
    "\n",
    "master  = pursuer_NN(input_size=4, n_actions=5, num_neurons_hidden=256, device=device).to(device)\n",
    "master  = torch.load('pursuer_NN_Env'+str(sel_env)+'_res'+str(res)+'.pt')\n",
    "master.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "\n",
    "    return r#(r - r.mean())# / (r.std() + np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE_PURSUER_FIXED(total_trajectories=2000, gamma=0.9, lr=0.001, batch_size=8, MAX_EPSILON = 1.0, MIN_EPSILON = 0.1, DECAY_RATE = 0.001):\n",
    "    # Set up lists to hold results\n",
    "    train_rewards  = []\n",
    "    train_rewards.append(0)\n",
    "    test_rewards   = []\n",
    "    test_rewards.append(0)\n",
    "    test_episodes  = []\n",
    "    test_episodes.append(0)\n",
    "\n",
    "    batch_states            = []\n",
    "    batch_rewards           = []\n",
    "    batch_total_rewards     = []\n",
    "    batch_discount_actions  = []\n",
    "    batch_counter           = 0\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(pursuer_model_training.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(total_trajectories):\n",
    "        s_0 = env.reset_fixed_initial_position()\n",
    "\n",
    "        states          = []\n",
    "        actions         = []\n",
    "        rewards         = []\n",
    "\n",
    "        done = False\n",
    "        t = 0\n",
    "\n",
    "        \n",
    "        while not done:\n",
    "            epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * ep)\n",
    "            if np.random.rand() < epsilon:\n",
    "                p_dist = master(s_0).detach().cpu().numpy()\n",
    "            else:\n",
    "                p_dist = pursuer_model_training(s_0).detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "            s_1, r, done, a_p = env.step_training(p_dist, t)\n",
    "            \n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(a_p)\n",
    "\n",
    "            t += 1\n",
    "            s_0 = s_1\n",
    "\n",
    "            if done:\n",
    "                batch_states.extend(states)\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_total_rewards.append(sum(rewards))\n",
    "                batch_discount_actions.extend(actions)\n",
    "\n",
    "                batch_counter += 1\n",
    "\n",
    "                if batch_counter ==  batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    state_tensor    = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor   = torch.FloatTensor(batch_rewards).to(device)\n",
    "                    action_tensor   = torch.LongTensor(batch_discount_actions)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    logprob             = torch.log(pursuer_model_training(state_tensor)).to(device)\n",
    "                    selected_logprobs   = reward_tensor * logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "                    loss                = -selected_logprobs.mean()\n",
    "\n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "\n",
    "                    batch_rewards   = []\n",
    "                    batch_discount_actions   = []\n",
    "                    batch_states    = []\n",
    "                    batch_counter   = 0\n",
    "\n",
    "                    # Test the neural network with the best action\n",
    "                    s_0 = env.reset_fixed_initial_position()\n",
    "                    rewards_test = 0\n",
    "                    done = False\n",
    "                    t = 0\n",
    "                    while not done:\n",
    "                        p_dist = pursuer_model_training(s_0).detach().cpu().numpy()\n",
    "                        s_1, r, done, _ = env.step_best_action(p_dist, t)\n",
    "                        rewards_test += r\n",
    "                        t += 1\n",
    "                        s_0 = s_1\n",
    "                    \n",
    "                    if rewards_test > test_rewards[-1]:\n",
    "                        pursuer_model.load_state_dict(pursuer_model_training.state_dict())\n",
    "                        test_rewards.append(rewards_test)\n",
    "                    else:\n",
    "                        test_rewards.append(test_rewards[-1])\n",
    "                        \n",
    "                    train_rewards.append(np.mean(batch_total_rewards))\n",
    "                    \n",
    "                    batch_total_rewards = []\n",
    "                    test_episodes.append(ep)\n",
    "\n",
    "                    print('\\rEp: {}, average training rewards {:.4f}, test rewards {:.4f} '.format(ep+1, train_rewards[-1], test_rewards[-1]), end='')\n",
    "\n",
    "    return test_episodes, train_rewards, test_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 4404, average training rewards 8.0000, test rewards 8.0000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3400/2208495332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREINFORCE_PURSUER_FIXED\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_trajectories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3400/2624351077.py\u001b[0m in \u001b[0;36mREINFORCE_PURSUER_FIXED\u001b[0;34m(total_trajectories, gamma, lr, batch_size, MAX_EPSILON, MIN_EPSILON, DECAY_RATE)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0ms_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Game based deep reinforcement learning for target tracking/3 Motion Policies Improvement last last approach/One initial position/environment.py\u001b[0m in \u001b[0;36mstep_training\u001b[0;34m(self, pursuer_dsit, t, epsilon)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobstacle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mobstacle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencloses_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                     \u001b[0mobstacle_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/polygon.py\u001b[0m in \u001b[0;36mencloses_point\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \"\"\"\n\u001b[1;32m    735\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/polygon.py\u001b[0m in \u001b[0;36msides\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/line.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, p1, p2, **kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSegment2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSegment3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/line.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, p1, p2, **kwargs)\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLinearEntity2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_svg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#66cc99\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/line.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, p1, p2, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# sometimes we return a single point if we are not given two unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/point.py\u001b[0m in \u001b[0;36m_normalize_dimension\u001b[0;34m(cls, *points, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if no dim was given, use the highest dimensional point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mambient_dimension\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mambient_dimension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/point.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if no dim was given, use the highest dimensional point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mambient_dimension\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mambient_dimension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/point.py\u001b[0m in \u001b[0;36mambient_dimension\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mambient_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;34m\"\"\"Number of components this point has.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_ambient_dimension'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/geometry/point.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sympy/core/basic.py\u001b[0m in \u001b[0;36margs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    740\u001b[0m         change the interface in the future if needed).\n\u001b[1;32m    741\u001b[0m         \"\"\"\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_episodes, train_rewards, test_rewards = REINFORCE_PURSUER_FIXED(total_trajectories=12000, batch_size=12, lr=0.0005, MIN_EPSILON=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.plot(test_episodes, train_rewards, label='Mean reward in training at the end of batch')\n",
    "plt.plot(test_episodes, test_rewards, label='Reward with best policy at the end of batch')\n",
    "\n",
    "window = 10\n",
    "smoothed_train_rewards = [np.mean(train_rewards[i-window:i+1]) if i > window \n",
    "                    else np.mean(train_rewards[:i+1]) for i in range(len(train_rewards))]\n",
    "plt.plot(test_episodes, smoothed_train_rewards, 'k')\n",
    "\n",
    "plt.xlabel('Number of trajectories in training')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Training mean rewards and best policy rewards')\n",
    "plt.legend()\n",
    "#grid\n",
    "yticks = np.linspace(0, test_rewards[-1], test_rewards[-1]+1)\n",
    "ax.set_yticks(yticks)\n",
    "plt.grid()\n",
    "plt.savefig('pursuer_fixed_initial_position_Supervisor_' + str(sel_env) + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pursuer_model,  'pursuer_RL_supervisor_Env' + str(sel_env) + '_res' + str(res) + '.pt')\n",
    "torch.save(pursuer_model.state_dict(),  'pursuer_dict_RL_supervisor_Env' + str(sel_env) + '_res' + str(res) + '.pt')\n",
    "\n",
    "np.savetxt('pursuer_RL_Env' + str(sel_env) + '_res' + str(res) + '_train_rewrds_e-greedy_Supervisor.txt', train_rewards, fmt='%.4f')\n",
    "np.savetxt('pursuer_RL_Env' + str(sel_env) + '_res' + str(res) + '_test_rewrdse-greedy_Supervisor.txt', test_rewards, fmt='%.4f')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
